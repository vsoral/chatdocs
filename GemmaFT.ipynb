{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsoral/chatdocs/blob/main/GemmaFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio\n",
        "\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "!pip install -U --no-deps unsloth"
      ],
      "metadata": {
        "id": "n-JVAHE-tDBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys;\n",
        "from unsloth import FastModel, FastVisionModel\n",
        "import torch\n",
        "import platform\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "import gc\n",
        "import time"
      ],
      "metadata": {
        "id": "03wfg93SutGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if 'inputs' in globals(): del globals()['inputs']\n",
        "    if 'model' in globals(): del globals()['model']\n",
        "    if 'processor' in globals(): del globals()['processor']\n",
        "    if 'trainer' in globals(): del globals()['trainer']\n",
        "    if 'peft_model' in globals(): del globals()['peft_model']\n",
        "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "os.environ['CURL_CA_BUNDLE'] = ''"
      ],
      "metadata": {
        "id": "2hsgM_f9uxE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_memory()"
      ],
      "metadata": {
        "id": "Cn_Zfqj7vDHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name = \"/content/drive/MyDrive/FineTuning/gemma-3-4b-it\",\n",
        "    max_seq_length = 2048, # Choose any for long context!\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    local_files_only=True\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")\n",
        "\n",
        "\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    finetune_vision_layers     = True, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    r = 8,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 8,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False\n",
        ")\n",
        "\n",
        "#model.to(torch.float32)   ##comment this for now."
      ],
      "metadata": {
        "id": "OKy-vIlYvGKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"You are a Language Model also specialized in interpreting visual data from chart images.\n",
        "Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n",
        "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\"\n",
        "\n",
        "d1 = load_dataset(\"imagefolder\", data_dir=\"/content/drive/MyDrive/FineTuning/hf_imgdb\")\n",
        "\n",
        "def format_data(sample):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_message\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"image\": sample[\"image\"],\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"Get the bounding box coordinates in the image where the plot resembles the shape: \" + sample['objects']['labels'][0],\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": str(sample['objects']['bbox'][0])\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "training_dataset = [format_data(sample) for sample in d1['train']]\n",
        "testing_dataset = [format_data(sample) for sample in d1['test']]"
      ],
      "metadata": {
        "id": "iCIjUf7lvXcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "\n",
        "print(\"B16 : \" + str(is_bf16_supported()))\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
        "    train_dataset = training_dataset,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 70,\n",
        "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = False, # Changed to False\n",
        "        bf16 = False, # Changed to False\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",     # For Weights and Biases\n",
        "\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "        dataset_text_field = \"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc = 4,\n",
        "        max_seq_length = 2048,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "RMXceRDcvdMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n",
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "torch.cuda.device.st\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")"
      ],
      "metadata": {
        "id": "SEtZFeoNvpp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/FineTuning/gemma-3-4b-it/gemma_3b_trained\")  # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/FineTuning/gemma-3-4b-it/gemma_3b_trained\")"
      ],
      "metadata": {
        "id": "uDq-_z7Jv0gj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}